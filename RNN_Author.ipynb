{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Author   \n",
    "In this Notebook I trained a LSTM RNN model on the book Call Me By Your Name written by AndrÃ© Aciman.   \n",
    "![](https://upload.wikimedia.org/wikipedia/en/7/77/Call_Me_By_Your_Name%2C_2007_book_cover.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, take a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_path = './text/Call_Me_By_Your_Name.txt'\n",
    "with open(file_path, encoding='utf-8', errors='ignore') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PART 1\\n\\nIf Not Later, When?\\n\\n\\n\"Later!\" The word, the voice, the attitude.\\nI\\'d never heard anyone use'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique words (roughly): 11833\n",
      "number of sentences: 1813\n"
     ]
    }
   ],
   "source": [
    "print('number of unique words (roughly): {}'.format(len({word: None for word in text.split()})))\n",
    "sentences = [sentence for sentence in text.split('\\n') if len(sentence) > 0]\n",
    "print('number of sentences: {}'.format(len(sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data   \n",
    "In this section, I'll make a dictionary of vocabulary and it's corresponding index, and then convert all the words in the book to integers for the RNN model to take in. Punctuations should also be converted to symbols and put into training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the punctuation token look up dictionary\n",
    "punc_token = {'.': '||Period||',\n",
    "             ',': '||Comma||',\n",
    "             '\"': '||Quotation_Mark||',\n",
    "             ';': '||Semicolon||',\n",
    "             '!': '||Exclamation_Mark||',\n",
    "             '?': '||Question_Mark||',\n",
    "             '(': '||Left_Parentheses||',\n",
    "             ')': '||Right_Parentheses||',\n",
    "             '--': '||Dash||',\n",
    "             '\\n': '||Return||',}\n",
    "\n",
    "# Replace the punctuation in the text with the token\n",
    "for key, token in punc_token.items():\n",
    "        text = text.replace(key, ' {} '.format(token))\n",
    "\n",
    "text = text.lower()\n",
    "text = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vocab = sorted(set(text))\n",
    "\n",
    "vocab_to_int = {word: index for (index, word) in enumerate(set(word_vocab))}\n",
    "int_to_vocab = {index: word for (word, index) in vocab_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode the text, replace the word with index\n",
    "encoded_text = np.array([vocab_to_int[c] for c in text], dtype=np.int32)\n",
    "assert len(text) == len(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the previous work for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump((encoded_text, vocab_to_int, int_to_vocab, punc_token), open('preprocess.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the check point   \n",
    "load the data saved before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_text, vocab_to_int, int_to_vocab, punc_token = pickle.load(open('preprocess.p', mode='rb'))\n",
    "vocab_size = len(vocab_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "I'll split the network to different parts first and then call them as functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input placeholder\n",
    "create the tensorflow placeholders for the input, target and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_input_placeholder():\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    target = tf.placeholder(tf.int32, [None, None], name='target')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    return inputs, target, lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer\n",
    "For every input, it's like one_hot encodding, with only one value valid in the whole word vocabulary vector   \n",
    "The matrix multiplication going into the first hidden layer will have almost all of the resulting values be zero. This a huge waste of computation.    \n",
    "So I use the embedding layer for a look up table with the function    \n",
    "      \n",
    "`tf.nn.embedding_lookup()`      \n",
    "the embed dim means the number of embedding dimentions\n",
    "![](./img/embedding.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding_layer(input_data, vocab_size, embed_dim):\n",
    "    embedding = tf.Variable(tf.random_uniform([vocab_size, embed_dim], -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, input_data)\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Cell\n",
    "use `tf.contrib.rnn.BasicLSTMCell` and `tf.contrib.rnn.MultiRNNCell` to build the LSTM cell\n",
    "![](./img/lstm.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lstm_cell(lstm_size, num_layers, batch_size, keep_prob = 1.0):\n",
    "    def build_cell(lstm_size, keep_prob):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "       \n",
    "        # Add dropout to the cell\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "    \n",
    "    # Stack up multiple LSTM layers\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "    initial_state = tf.identity(cell.zero_state(batch_size, tf.float32), name='initial_state')\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN layer\n",
    "use `tf.nn.dynamic_rnn()`  to create a recurrent neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    output, state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "    final_state = tf.identity(state, name='final_state')\n",
    "    return output, final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the network with one fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_logits(embedding, cell, vocab_size):\n",
    "    output, final_state = build_rnn(cell, embedding)\n",
    "    logits = tf.contrib.layers.fully_connected(output, vocab_size)\n",
    "    return logits, final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batches of Data\n",
    "To feed the network, the input data and taget should be formed to ![](./img/format.png)    \n",
    "And the way to do this is ![](./img/method.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    # remove the last batch which cannot be filled\n",
    "    n_batch = len(int_text) // (batch_size*seq_length)\n",
    "    int_text = int_text[:n_batch*batch_size*seq_length]\n",
    "    text_shift = np.zeros_like(int_text)\n",
    "    text_shift[:-1], text_shift[-1] = int_text[1:], int_text[0]\n",
    "\n",
    "    text_reshape = np.array(int_text).reshape(batch_size, -1)\n",
    "    text_shift_reshape = text_shift.reshape(batch_size, -1)\n",
    "\n",
    "    final = []\n",
    "    for i in range(n_batch):\n",
    "        batch_input = text_reshape[:, i*seq_length:(i+1)*seq_length].reshape(batch_size, seq_length)\n",
    "        batch_target = text_shift_reshape[:, i*seq_length:(i+1)*seq_length].reshape(batch_size, seq_length)\n",
    "        combine = np.array([batch_input, batch_target])\n",
    "        final.append(combine)\n",
    "    return np.array(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 100\n",
    "# Batch Size\n",
    "batch_size = 128\n",
    "# Size of the hidden layers in the LSTM cells\n",
    "lstm_size = 256\n",
    "# Number of lstm cells in one multiple RNN cell\n",
    "num_layers = 1\n",
    "# Embedding Dimension Size\n",
    "embed_dim = 200\n",
    "# Sequence Length\n",
    "seq_length = 20\n",
    "# Learning Rate\n",
    "learning_rate = 0.01\n",
    "#Keep prob in hidden lstm cell\n",
    "keep_prob = 0.6\n",
    "\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 100\n",
    "\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    input_text, targets, lr = get_input_placeholder()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    embedding = get_embedding_layer(input_text, vocab_size, embed_dim)\n",
    "    cell, initial_state = get_lstm_cell(lstm_size, num_layers, input_data_shape[0], keep_prob)\n",
    "    logits, final_state = get_logits(embedding, cell, vocab_size)\n",
    "\n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/36   train_loss = 8.944\n",
      "Epoch   2 Batch   28/36   train_loss = 5.977\n",
      "Epoch   5 Batch   20/36   train_loss = 5.775\n",
      "Epoch   8 Batch   12/36   train_loss = 5.612\n",
      "Epoch  11 Batch    4/36   train_loss = 5.597\n",
      "Epoch  13 Batch   32/36   train_loss = 5.433\n",
      "Epoch  16 Batch   24/36   train_loss = 5.364\n",
      "Epoch  19 Batch   16/36   train_loss = 5.261\n",
      "Epoch  22 Batch    8/36   train_loss = 5.021\n",
      "Epoch  25 Batch    0/36   train_loss = 4.805\n",
      "Epoch  27 Batch   28/36   train_loss = 4.821\n",
      "Epoch  30 Batch   20/36   train_loss = 4.672\n",
      "Epoch  33 Batch   12/36   train_loss = 4.559\n",
      "Epoch  36 Batch    4/36   train_loss = 4.599\n",
      "Epoch  38 Batch   32/36   train_loss = 4.424\n",
      "Epoch  41 Batch   24/36   train_loss = 4.400\n",
      "Epoch  44 Batch   16/36   train_loss = 4.379\n",
      "Epoch  47 Batch    8/36   train_loss = 4.134\n",
      "Epoch  50 Batch    0/36   train_loss = 4.039\n",
      "Epoch  52 Batch   28/36   train_loss = 4.208\n",
      "Epoch  55 Batch   20/36   train_loss = 4.144\n",
      "Epoch  58 Batch   12/36   train_loss = 4.090\n",
      "Epoch  61 Batch    4/36   train_loss = 4.174\n",
      "Epoch  63 Batch   32/36   train_loss = 4.038\n",
      "Epoch  66 Batch   24/36   train_loss = 4.029\n",
      "Epoch  69 Batch   16/36   train_loss = 4.104\n",
      "Epoch  72 Batch    8/36   train_loss = 3.903\n",
      "Epoch  75 Batch    0/36   train_loss = 3.841\n",
      "Epoch  77 Batch   28/36   train_loss = 3.946\n",
      "Epoch  80 Batch   20/36   train_loss = 3.950\n",
      "Epoch  83 Batch   12/36   train_loss = 3.923\n",
      "Epoch  86 Batch    4/36   train_loss = 4.045\n",
      "Epoch  88 Batch   32/36   train_loss = 3.868\n",
      "Epoch  91 Batch   24/36   train_loss = 3.946\n",
      "Epoch  94 Batch   16/36   train_loss = 3.980\n",
      "Epoch  97 Batch    8/36   train_loss = 3.785\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(encoded_text, batch_size, seq_length)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump((seq_length, save_dir), open('params.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to Write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### need a function to get the tensor\n",
    "use `get_tensor_by_name()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    input_tensor = loaded_graph.get_tensor_by_name(\"input:0\")\n",
    "    initial_state = loaded_graph.get_tensor_by_name(\"initial_state:0\")\n",
    "    final_state = loaded_graph.get_tensor_by_name(\"final_state:0\")\n",
    "    prob_tensor = loaded_graph.get_tensor_by_name(\"probs:0\")\n",
    "    return input_tensor, initial_state, final_state, prob_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### and a function to choose a word from the probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    index = probabilities.argmax()\n",
    "    return int_to_vocab[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the previous trained model and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, vocab_to_int, int_to_vocab, punc_token = pickle.load(open('preprocess.p', mode='rb'))\n",
    "seq_length, load_dir = pickle.load(open('params.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ready to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oliver and, he were right?\" i asked.\n",
      "\" i don't know what i wanted.\"\n",
      "i didn't know what i was too much.\n",
      "\" and i know it,\" i said, to say, oliver, who had come with a hand his smile on the words that both didn't want to know how to last, i'd always had say this what i was happy, but that afternoon suddenly finally, i couldn't have been too long to get away him on the table, he saw his body would be the other. this was the first thing about san clemente.\"\n",
      "\" i was headed to sleep, you don't like to, he is said,\" he want,\" i know he was his shirt.\"\n",
      "\" i think how long i am when you don't know what things you were me,\" he said. the book? i asked.\n",
      "\" i may not know you.\"\n",
      "\" that's a way of looking over and i was going to say,\" i\n"
     ]
    }
   ],
   "source": [
    "gen_length = 200\n",
    "\n",
    "prime_word = 'oliver'\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [prime_word]\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "        \n",
    "        pred_word = pick_word(probabilities[dyn_seq_length-1], int_to_vocab)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    # Remove tokens\n",
    "    text = ' '.join(gen_sentences)\n",
    "    for key, token in punc_token.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        text = text.replace(' ' + token.lower(), key)\n",
    "    text = text.replace('\\n ', '\\n')\n",
    "    text = text.replace('( ', '(')\n",
    "        \n",
    "    print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
